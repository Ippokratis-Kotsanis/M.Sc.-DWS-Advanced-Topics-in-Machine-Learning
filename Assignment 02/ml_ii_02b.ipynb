{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["KSHtiWFE1p9e"],"gpuType":"T4","authorship_tag":"ABX9TyPn82rouCf6aRMt0qTMog42"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["ADVANCED TOPICS IN MACHINE LEARNING\n","\n","Assignment - 2\n","\n","Ιπποκράτης Κοτσάνης - 131\n","\n","Φιλίτσα-Ιωάννα Κουσκουβέλη - 125\n","\n","PART B"],"metadata":{"id":"XwKD2zuH1hUn"}},{"cell_type":"markdown","source":["**Sources:**\n","\n","    https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text\n","    https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n","    https://www.analyticsvidhya.com/blog/2021/01/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning/\n","    https://pypi.org/project/kneed/\n","    https://github.com/arvkevi/kneed\n","    https://machinelearningmastery.com/repeated-k-fold-cross-validation-with-python/\n","\n","**Links used:**\n","\n","    https://pythonprogramminglanguage.com/kmeans-text-clustering/\n","    https://www.w3schools.com/python/python_ml_k-means.asp\n","    https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/\n"],"metadata":{"id":"MIKcWLwPbZwx"}},{"cell_type":"markdown","source":["#**Import the necessary libraries and modules** "],"metadata":{"id":"KSHtiWFE1p9e"}},{"cell_type":"code","source":["!pip install kneed"],"metadata":{"id":"r_aa9UQPe8pr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684341428503,"user_tz":-180,"elapsed":4661,"user":{"displayName":"Φιλίτσα Ιωάννα Κουσκουβέλη","userId":"11927878089400920734"}},"outputId":"2327f59d-4756-4fa7-ed61-a74d09b421a9"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: kneed in /usr/local/lib/python3.10/dist-packages (0.8.3)\n","Requirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from kneed) (1.22.4)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from kneed) (1.10.1)\n"]}]},{"cell_type":"code","execution_count":15,"metadata":{"id":"I4bCBRknHxZO","executionInfo":{"status":"ok","timestamp":1684341428505,"user_tz":-180,"elapsed":25,"user":{"displayName":"Φιλίτσα Ιωάννα Κουσκουβέλη","userId":"11927878089400920734"}}},"outputs":[],"source":["import warnings\n","from sklearn.exceptions import ConvergenceWarning, UndefinedMetricWarning, FitFailedWarning\n","\n","import pandas as pd \n","import re\n","import matplotlib.pyplot as plt\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer # https://pypi.org/project/sklearn-features/\n","from sklearn.cluster import KMeans\n","from kneed import KneeLocator\n","\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, zero_one_loss\n"," "]},{"cell_type":"markdown","source":["# **Functions**"],"metadata":{"id":"GmL6YK3m13nf"}},{"cell_type":"code","source":["def load_vocab_dict(vocabFileName):\n","    vocab = {}\n","    with open(vocabFileName, 'r') as f:\n","        for line in f:\n","            word, index = line.strip().split(',')\n","            vocab[word] = int(index)\n","    return vocab\n","\n","\n","def txts_to_bOs(fileName):\n","    '''\n","    Takes as input the file's name of our data's file. Each line corresponds to a text. \n","    Each text comprises of a different number of sentences. Each sentence comprises \n","    of a different number of words. \n","\n","    It returns a list of strings. Each string correspends to a text's sentences.\n","    Words are separated with simple spaces one from another.\n","    '''\n","    # Read data from file\n","    with open(fileName, 'r') as f:\n","        lines = f.readlines()  \n","\n","    dataset = []\n","    pattern = r'<\\d+>\\s([\\d\\s]+)'\n","    for line in lines:\n","        txt_in_sentences = re.findall(pattern, line)\n","        txt = ''.join(txt_in_sentences) #join the strings of different sentences to a single one\n","        dataset.append(txt.rstrip()) #strip pf any characters in the end\n","    return dataset\n","\n","\n","def txts_to_sentences(fileName):\n","    '''\n","    Takes as input the file's name of our data's file. Each line corresponds to a text. \n","    Each text comprises of a different number of sentences. Each sentence comprises \n","    of a different number of words. \n","\n","    It returns a list of strings. Each string correspends to a text's sentence.\n","    Words are separated with simple spaces one from another.\n","    '''\n","    # Read data from file\n","    with open(fileName, 'r') as f:\n","        lines = f.readlines()\n","\n","    dataset, idxs_list = [], []\n","\n","    idx_pattern = r\"<(\\d+)>\"\n","    sentence_pattern = r\"<\\d+>\\s((?:\\d+\\s)+)\"\n","\n","    for line in lines:\n","        idx = re.search(idx_pattern, line) #; print(idx.group(1))\n","        idxs_list.append(int(idx.group(1))) \n","        sentences = re.findall(sentence_pattern, line)\n","        sentences = [s.strip() for s in sentences] #; print(sentences)\n","        dataset+= sentences\n","    return idxs_list, dataset\n","\n","\n","def vectorize_inputData(trainData, testData, vectorizer):  \n","\n","    trainData_matrix = vectorizer.fit_transform(trainData)\n","    testData_matrix = vectorizer.transform(testData)\n","    #print('skata')\n","    #print(type(trainData_matrix), trainData_matrix.shape)\n","    return trainData_matrix, testData_matrix\n","\n","\n","def plot_elbow(start, stop, inertias_list):    \n","    plt.plot(range(start, stop), inertias_list, marker='o')\n","    plt.title('Elbow method')\n","    plt.xlabel('Number of clusters')\n","    plt.ylabel('Inertia')\n","    plt.show()\n","    plt.clf()\n","    plt.close()\n","\n","\n","def search_opt_k_kmeans(data_matrix, max_num_k, vectorizer, num_centroids = 10, numIter=100, elbowPlot_flag = True):\n","    \n","    top_centroids_lol = []\n","    inertias_list = []\n","    candidate_k_list = [i for i in range(2,max_num_k)]\n","\n","    for k in candidate_k_list:\n","        model = KMeans(n_clusters=k, init='k-means++', max_iter=numIter, n_init=1)\n","        model.fit(data_matrix)\n","\n","        #print(\"Top terms per cluster:\")\n","        order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n","        terms = vectorizer.get_feature_names_out()\n","        curren_k_list = []\n","        for i in range(k):\n","            #print(\"Cluster %d:\" % i),\n","            cluster_top = []\n","            for ind in order_centroids[i, :num_centroids]:\n","                #print(' %s' % terms[ind]),\n","                cluster_top.append(terms[ind]) # creating a list of each cluster's top centroids   \n","            curren_k_list.append(cluster_top) # a list containing the lists of each cluster's top centroids for k value = k \n","\n","        inertias_list.append(model.inertia_)            \n","        top_centroids_lol.append(curren_k_list) # list of max_num_k-2 elements, each element coresponds to a search of k, each element is a list of lists\n","    \n","    kneedle = KneeLocator(candidate_k_list, inertias_list, curve=\"convex\", direction=\"decreasing\")\n","    opt_k = kneedle.knee #; print(opt_k)\n","    if opt_k == None:\n","        return None, None\n","    \n","    idx_opt = candidate_k_list.index(opt_k)\n","\n","    if elbowPlot_flag:\n","        plot_elbow(2,max_num_k, inertias_list)\n","\n","    return opt_k, top_centroids_lol[idx_opt]\n","\n","\n","def txts_to_clusterVectors(model, data_matrix, opt_k, idxs_list):\n","    \n","    data_transf = model.predict(data_matrix)\n","    #print(type(train_data_transf), train_data_transf.shape)\n","    \n","    data_transf_list = data_transf.tolist()\n","    df = pd.DataFrame(columns=[str(i) for i in range(1, (opt_k+1))]) #; print(train_df)\n","\n","    for idx in idxs_list:\n","        \n","        if idx>len(data_transf_list):\n","            idx = len(data_transf_list)\n","\n","        txt_clusters = data_transf_list[:idx]\n","        txt_clusters_dict = dict(list(zip([str(i) for i in range(1, (opt_k+1))], [ [0] for i in range(1, (opt_k+1))])))\n","        \n","        for cluster in txt_clusters:\n","            txt_clusters_dict[str(cluster+1)][0]+=1 \n","\n","        new_row = pd.DataFrame(txt_clusters_dict)\n","        df = pd.concat([df, new_row], ignore_index=True)\n","        \n","        if data_transf_list:\n","            data_transf_list = data_transf_list[idx:]\n","        else:\n","            break\n","    \n","    print('clustered data:\\n', df.head(100)); print('clustered data shape:\\n', df.shape)\n","\n","    return df\n","\n","\n","def get_targValues(fileName):\n","    \n","    df = pd.read_csv(fileName, sep='\\s+', header=None)#; print(df.head())\n","    value_counts = df.apply(lambda x: x.eq(1).sum()) #; print(value_counts, type(value_counts))\n","    max_index = value_counts.idxmax(); print('most frequent class: ', (max_index+1))\n","    return max_index, df[max_index]\n","\n","\n","def get_txt_targVal_from_sentences(fileName, class_idx):\n","\n","    df = pd.read_csv(fileName, sep='\\s+', header=None)\n","    df = df[[0,(class_idx)]]\n","    txt_idx, targVal = -1, 0\n","    idx_list, target_list = [], []\n","\n","    for i in range(df.shape[0]):\n","\n","        if df.iloc[i, 1] and txt_idx==df.iloc[i, 0]:\n","            continue\n","\n","        if df.iloc[i, 1]:\n","            targVal=df.iloc[i, 1]\n","\n","        if txt_idx!=df.iloc[i, 0]:\n","            target_list.append(targVal)\n","            idx_list.append(df.iloc[i, 0])\n","            txt_idx=df.iloc[i, 0]\n","            targVal=0\n","\n","    #print(idx_list)\n","    #print(target_list)\n","    return idx_list, target_list\n","\n","\n","def get_clfs():\n","    # get a list of models to evaluate\n","    clfs_dict = dict()\n","\n","    # Logistic Regression Classifier Grid Search\n","    clfs_dict['lr'] = GridSearchCV(LogisticRegression(max_iter=10000), {'penalty': ['l2'], 'C': [0.1, 1, 10, 100]}, cv=5)\n","\n","    # Linear SVM Classifier Grid Search\n","    clfs_dict['lsvc'] = GridSearchCV(LinearSVC(max_iter=100000), {'C': [0.1, 1, 10, 100]}, cv=5)\n","     \n","    # Gradient Boosting Classifier (base)\n","    clfs_dict['gbm'] = GradientBoostingClassifier()\n","    \n","    # Random Forest Classifier (base)\n","    clfs_dict['rf'] = RandomForestClassifier()\n","\n","    return clfs_dict\n","\n","\n","def apply_clf_models(clfs_dict, X_train, y_train, X_test, y_test):\n","    for clf_name, clf in clfs_dict.items():\n","      clf.fit(X_train, y_train)\n","      pred_labels = clf.predict(X_test)\n","\n","      print('\\nEvaluating {}'.format(clf_name))\n","      print(classification_report(y_test, pred_labels))\n","      print('Subset accuracy = {}'.format((1 - zero_one_loss(y_test, pred_labels))))\n","\n","\n","def select_head_records(n, df_X_train, df_y_train, df_X_test, df_y_test):\n","    df_X_train = df_X_train.head(n)\n","    df_y_train = df_y_train.head(n)\n","    df_X_test = df_X_test.head(int(n/2))\n","    df_y_test = df_y_test.head(int(n/2))\n","    \n","    return df_X_train, df_y_train, df_X_test, df_y_test\n","\n","\n"],"metadata":{"id":"5ZzL-Fur2KUu","executionInfo":{"status":"ok","timestamp":1684341428513,"user_tz":-180,"elapsed":30,"user":{"displayName":"Φιλίτσα Ιωάννα Κουσκουβέλη","userId":"11927878089400920734"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["    # Data files' names\n","    x_train_fileName, x_test_fileName, = 'train-data.dat', 'test-data.dat' # X \n","    y_train_fileName, y_test_fileName = 'train-label.dat', 'test-label.dat' # y\n","\n","    vectorizer = TfidfVectorizer()\n","\n","    # Read y_train data and transform the multi-labeled output to binary \n","    class_idx, df_y_train = get_targValues(y_train_fileName) # class_idx: index of the most common class \n","    #print('y_train_fileName',df_y_train.shape)\n","\n","    # Read y_test data and transform the multi-labeled output to binary \n","    df_y_test = pd.read_csv(y_test_fileName, sep='\\s+', header=None)\n","    df_y_test = df_y_test[class_idx] #; print('y_test_fileName',df_y_test.shape)\n","    "],"metadata":{"id":"mM6zuLQ2b6FY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684341428514,"user_tz":-180,"elapsed":30,"user":{"displayName":"Φιλίτσα Ιωάννα Κουσκουβέλη","userId":"11927878089400920734"}},"outputId":"17906ebd-d72f-4b21-a690-df48211abc7d"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["most frequent class:  3\n"]}]},{"cell_type":"markdown","source":["Εντοπίζεται η συχνότερη από τις 20 κλάσεις και μετασχιματίζονται το y_train και το y_test, ώστε να αντιμετωπίστει ένα \n","δυαδικό πρόβλημα ταξινόμησης."],"metadata":{"id":"uMup8eVCuixN"}},{"cell_type":"markdown","source":["# **02b: Task 01**"],"metadata":{"id":"m9yAHEj42K-H"}},{"cell_type":"code","source":["    # Read X_train data\n","    idxs_X_train_list, X_train = txts_to_sentences(x_train_fileName) #; print(type(X_train[0]),type(X_train))\n","    #print(X_train[0:1])\n","\n","    # Read X_test data\n","    idxs_X_test_list, X_test = txts_to_sentences(x_test_fileName)\n","    \n","    # Vectorize X_train data and X_test data\n","    trainData_matrix, testData_matrix = vectorize_inputData(X_train, X_test, vectorizer)\n","\n"],"metadata":{"id":"xzjSfitq17II","executionInfo":{"status":"ok","timestamp":1684341431923,"user_tz":-180,"elapsed":3434,"user":{"displayName":"Φιλίτσα Ιωάννα Κουσκουβέλη","userId":"11927878089400920734"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["    # k-means search to find optimum k\n","    max_num_k = 11\n","    numIter = 100\n","    opt_k = None\n","    while(opt_k == None):\n","        opt_k, top_centroids_lol = search_opt_k_kmeans(trainData_matrix, max_num_k, vectorizer, elbowPlot_flag=False)\n","    \n","    # Create k-means model with the optimum k on training set examples\n","    model = KMeans(n_clusters=opt_k, init='k-means++', max_iter=numIter, n_init=1)\n","    model.fit(trainData_matrix)\n","\n","    print('optimum k: ', opt_k)\n","    print (type(trainData_matrix), trainData_matrix.shape)\n","    "],"metadata":{"id":"tr9iDM24chmI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684341441389,"user_tz":-180,"elapsed":9478,"user":{"displayName":"Φιλίτσα Ιωάννα Κουσκουβέλη","userId":"11927878089400920734"}},"outputId":"d55288d9-5c0c-41b3-b57a-fe6027013171"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["optimum k:  5\n","<class 'scipy.sparse._csr.csr_matrix'> (149925, 8510)\n"]}]},{"cell_type":"code","source":["    # Transform X_train data via clustering\n","    df_X_train = txts_to_clusterVectors(model, trainData_matrix, opt_k, idxs_X_train_list) #; print('trainData_matrix', df_X_train.shape)\n","\n","    # Transform X_test data via clustering\n","    df_X_test = txts_to_clusterVectors(model, testData_matrix, opt_k, idxs_X_test_list) #; print('testData_matrix',df_X_test.shape)\n","\n","    '''\n","    n = 200\n","    df_X_train, df_y_train, df_X_test, df_y_test = select_head_records(n, df_X_train, df_y_train, df_X_test, df_y_test)\n","    #'''\n","    "],"metadata":{"id":"a9-LCiTpcmDO","colab":{"base_uri":"https://localhost:8080/","height":625},"executionInfo":{"status":"ok","timestamp":1684341467616,"user_tz":-180,"elapsed":26234,"user":{"displayName":"Φιλίτσα Ιωάννα Κουσκουβέλη","userId":"11927878089400920734"}},"outputId":"83b69e53-9406-4700-e291-ca3b141f0e60"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["clustered data:\n","     1  2  3  4   5\n","0   0  0  0  1   1\n","1   1  0  0  1  26\n","2   0  3  1  1  26\n","3   0  0  0  0   2\n","4   0  0  0  0   7\n",".. .. .. .. ..  ..\n","95  1  1  0  4  25\n","96  0  1  0  0  27\n","97  8  2  3  0  18\n","98  0  0  0  2   9\n","99  0  0  0  1  30\n","\n","[100 rows x 5 columns]\n","clustered data shape:\n"," (8251, 5)\n","clustered data:\n","     1  2  3  4   5\n","0   2  3  2  1  23\n","1   0  2  1  0  28\n","2   1  1  0  0  29\n","3   0  1  0  3  15\n","4   0  0  0  0   4\n",".. .. .. .. ..  ..\n","95  0  2  1  2  25\n","96  0  1  0  0   6\n","97  1  0  1  0  10\n","98  0  0  0  0   7\n","99  0  0  0  0   4\n","\n","[100 rows x 5 columns]\n","clustered data shape:\n"," (3983, 5)\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\nn = 200\\ndf_X_train, df_y_train, df_X_test, df_y_test = select_head_records(n, df_X_train, df_y_train, df_X_test, df_y_test)\\n#'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["Στο *DeliciousMIL: A Data Set for Multi-Label Multi-Instance Learning with Instance Labels Data Set* dataset ομαδοποιήθηκαν (με k-means clustering) οι προτάσεις του συνόλου εκπαίδευσης. Για το βελτιστο k που βρέθηκε (και με βαση τα clusters του training set) κάθε έγγραφο του training set και ακολούθως του testing set αναπαραστάθηκε με βάση τις ομάδες στις οποίες ανήκαν οι προτάσεις του."],"metadata":{"id":"7kmczW0Ot94P"}},{"cell_type":"code","source":["    clfs_dict = get_clfs()\n","    apply_clf_models(clfs_dict, df_X_train, df_y_train, df_X_test, df_y_test)"],"metadata":{"id":"EZkzmKyFcyAI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684341865210,"user_tz":-180,"elapsed":397651,"user":{"displayName":"Φιλίτσα Ιωάννα Κουσκουβέλη","userId":"11927878089400920734"}},"outputId":"78ce6970-b6fe-4cc5-f19e-d3585f6b1cd5"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Evaluating lr\n","              precision    recall  f1-score   support\n","\n","           0       0.61      1.00      0.76      2425\n","           1       1.00      0.00      0.00      1558\n","\n","    accuracy                           0.61      3983\n","   macro avg       0.80      0.50      0.38      3983\n","weighted avg       0.76      0.61      0.46      3983\n","\n","Subset accuracy = 0.6090886266633191\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Evaluating lsvc\n","              precision    recall  f1-score   support\n","\n","           0       0.61      1.00      0.76      2425\n","           1       1.00      0.00      0.00      1558\n","\n","    accuracy                           0.61      3983\n","   macro avg       0.80      0.50      0.38      3983\n","weighted avg       0.76      0.61      0.46      3983\n","\n","Subset accuracy = 0.6090886266633191\n","\n","Evaluating gbm\n","              precision    recall  f1-score   support\n","\n","           0       0.61      0.98      0.75      2425\n","           1       0.47      0.03      0.05      1558\n","\n","    accuracy                           0.61      3983\n","   macro avg       0.54      0.50      0.40      3983\n","weighted avg       0.56      0.61      0.48      3983\n","\n","Subset accuracy = 0.6075822244539292\n","\n","Evaluating rf\n","              precision    recall  f1-score   support\n","\n","           0       0.61      0.82      0.70      2425\n","           1       0.38      0.17      0.23      1558\n","\n","    accuracy                           0.57      3983\n","   macro avg       0.49      0.50      0.47      3983\n","weighted avg       0.52      0.57      0.52      3983\n","\n","Subset accuracy = 0.5664072307306051\n"]}]},{"cell_type":"markdown","source":["Τα παραπάνω αποτελέσματα αξιολόγησης προέκυψαν αφού εφαρομόστηκε Grid Search για τους αλγορίθμους-κατηγοριοποιητές LogisticRegression (lr) και LinearSVM (lsvc) (με παραμέτρους {'penalty': ['l2'], 'C': [0.1, 1, 10, 100]} και {'C': [0.1, 1, 10, 100]} αντίστοιχα) και τα base models για τους GradientBoostingClassifier και RandomForestClassifier.\n","\n","Τόσο η Logistic Regression όσο και τα Linear SVMs σημείωσαν το ίδιο accuracy στο testing set (περίπου 60,9%). Ωστόσο, εξετάζοντας, επίσης, τις μετρικές precision, recall και F1, τα μοντέλα έχουν για τη \"θετική\" κατηγορία (δηλαδή ανήκουν στην συχνότερη κλάση) με recall 0,00 και βαθμολογία F1 0,00. Αυτό συνεπάγεται πως τα μοντέλα \"δυσκολεύονται\" να αναγνωρίσουν σωστά τις περιπτώσεις που ανήκουν στη \"θετική\" κατηγορία.\n","\n","Το μοντέλο Classifier Boosting Gradient σημειώνει accuracy στο testing set παρόμοια με εκείνη των LogisticRegression και Linear SVMs  (περίπου 60,8%). Ωστόσο, αποδίδει ελαφρώς καλύτερα όσον αφορά τις μετρικές recall και F1, σε σύγκριση με τα  LogisticRegression και Linear SVMs. Επιτυγχάνει recall 0,03 και βαθμολογία F1 0,05 για τη \"θετική\" κατηγορία.\n","\n","Το mοντέλο Random Forest Classifier σημειώνει το χαμηλότερο μεταξύ των μοντέλων accuracy στο testing set (περίπου 56,6%).  Ωστόσο, αποδίδει καλύτερα όσον αφορά τις μετρικές recall και F1, σε σύγκριση με τα  LogisticRegression, Linear SVMs και Classifier Boosting Gradient. Επιτυγχάνει  recall 0,17 και βαθμολογία F1 0,23 για τη \"θετική\" κατηγορία.\n","\n"],"metadata":{"id":"py2yvXRZp1eR"}},{"cell_type":"markdown","source":["# **02b: Task 02**"],"metadata":{"id":"cEhWnFYKdNTD"}},{"cell_type":"code","source":["    X_train = txts_to_bOs(x_train_fileName)\n","    #print(X_train[0:1])\n","    X_test = txts_to_bOs(x_test_fileName)\n","    trainData_matrix, testData_matrix = vectorize_inputData(X_train, X_test, vectorizer)\n","\n","    #print(type(trainData_matrix), type(testData_matrix))\n","    #print(trainData_matrix.shape,testData_matrix.shape)\n","\n","    df_X_train = pd.DataFrame(trainData_matrix.toarray())\n","    df_X_test = pd.DataFrame(testData_matrix.toarray())\n","\n","    '''\n","    n = 200\n","    df_X_train, df_y_train, df_X_test, df_y_test = select_head_records(n, df_X_train, df_y_train, df_X_test, df_y_test)\n","    '''\n","    #df_X_train, df_y_train, df_X_test, df_y_test = select_head_records(n, df_X_train, df_y_train, df_X_test, df_y_test)\n","\n"],"metadata":{"id":"NbAK8peGdTkY","executionInfo":{"status":"ok","timestamp":1684341868444,"user_tz":-180,"elapsed":3270,"user":{"displayName":"Φιλίτσα Ιωάννα Κουσκουβέλη","userId":"11927878089400920734"}},"colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"55029731-d8ec-4bb5-ebe0-6cf4eb3ea761"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nn = 200\\ndf_X_train, df_y_train, df_X_test, df_y_test = select_head_records(n, df_X_train, df_y_train, df_X_test, df_y_test)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["Στο *DeliciousMIL: A Data Set for Multi-Label Multi-Instance Learning with Instance Labels Data Set* dataset κάθε έγγραφο αναπαρίσταται με βάση όλες τις προτάσεις του. "],"metadata":{"id":"CaW67Qqa1S47"}},{"cell_type":"code","source":["    clfs_dict = get_clfs()\n","    apply_clf_models(clfs_dict, df_X_train, df_y_train, df_X_test, df_y_test)\n"," \n","    #randSearch_clf_models(df_X_train, df_y_train, df_X_test, df_y_test)\n","    "],"metadata":{"id":"M10dypEteH65","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2fe6e47e-ad81-42fe-9695-9ae609ab68e8","executionInfo":{"status":"ok","timestamp":1684342515784,"user_tz":-180,"elapsed":647349,"user":{"displayName":"Φιλίτσα Ιωάννα Κουσκουβέλη","userId":"11927878089400920734"}}},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Evaluating lr\n","              precision    recall  f1-score   support\n","\n","           0       0.69      0.87      0.77      2425\n","           1       0.66      0.40      0.49      1558\n","\n","    accuracy                           0.68      3983\n","   macro avg       0.67      0.63      0.63      3983\n","weighted avg       0.68      0.68      0.66      3983\n","\n","Subset accuracy = 0.6834044689932212\n","\n","Evaluating lsvc\n","              precision    recall  f1-score   support\n","\n","           0       0.69      0.87      0.77      2425\n","           1       0.66      0.38      0.49      1558\n","\n","    accuracy                           0.68      3983\n","   macro avg       0.68      0.63      0.63      3983\n","weighted avg       0.68      0.68      0.66      3983\n","\n","Subset accuracy = 0.682400200853628\n","\n","Evaluating gbm\n","              precision    recall  f1-score   support\n","\n","           0       0.65      0.91      0.76      2425\n","           1       0.64      0.25      0.36      1558\n","\n","    accuracy                           0.65      3983\n","   macro avg       0.65      0.58      0.56      3983\n","weighted avg       0.65      0.65      0.60      3983\n","\n","Subset accuracy = 0.6507657544564399\n","\n","Evaluating rf\n","              precision    recall  f1-score   support\n","\n","           0       0.66      0.91      0.77      2425\n","           1       0.66      0.26      0.37      1558\n","\n","    accuracy                           0.66      3983\n","   macro avg       0.66      0.59      0.57      3983\n","weighted avg       0.66      0.66      0.61      3983\n","\n","Subset accuracy = 0.6587998995731861\n"]}]},{"cell_type":"markdown","source":["Τα παραπάνω αποτελέσματα αξιολόγησης προέκυψαν αφού εφαρομόστηκε Grid Search για τους αλγορίθμους-κατηγοριοποιητές LogisticRegression (lr) και LinearSVM (lsvc) (με παραμέτρους {'penalty': ['l2'], 'C': [0.1, 1, 10, 100]} και {'C': [0.1, 1, 10, 100]} αντίστοιχα) και τα base models για τους GradientBoostingClassifier και RandomForestClassifier.\n","\n","Η Logistic Regression και τα Linear SVMs σημείωσαν παρόμοιο accuracy στο testing set (περίπου 68,3% και 68,2% αντίστοιχα). Οι τιμές των μετρικών precision, recall και F1 δεν διαφέρουν ιδιαίτερα μεταξύ των δύο μοντέλων. Τα μοντέλα \"αποδίδουν\" αρκετά καλά και για τη \"θετική\" κατηγορία (δηλαδή τη συχνότερη κλάση) και για την \"αρνητική\" κλάση, με ελαφρώς καλύτερη απόδοση για την \"αρνητική\" ετικέτα.\n","\n","Το μοντέλο Classifier Boosting Gradient σημειώνει accuracy στο testing set περίπου 65,1%. Σε ό,τι αφορά τις μετρικές precision, recall και F1 φαίνονται ιδιαίτερα υψηλές για την \"αρνητική\" ετικέτα. Επιπλέον, ειδικά η recall και η  F1 είναι υψηλότερες συγκριτικά με τη \"θετική\" ετικέτα, υποδηλώνοντας \"δυσκολία\" στην ακρίβεια αναγνώρισης περιπτώσεων που ανήκουν στη συχνότερη κλάση. \n","\n","Το mοντέλο Random Forest Classifier σημειώνει accuracy στο testing set περίπου 65,9%.  Παρόμοια με το μοντέλο Classifier Boosting Gradient, έχει καλή απόδοση στις precision, recall και F1 για την ετικέτα 0, αλλά η απόδοση για την ετικέτα 1 είναι σχετικά χαμηλότερη."],"metadata":{"id":"6Enbl_D4qcZi"}}]}