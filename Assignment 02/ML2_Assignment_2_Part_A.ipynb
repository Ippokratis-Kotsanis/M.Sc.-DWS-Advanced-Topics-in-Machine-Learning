{"cells":[{"cell_type":"markdown","metadata":{"id":"l7Ueg5rRc5sG"},"source":["ADVANCED TOPICS IN MACHINE LEARNING\n","\n","Assignment - 2\n","\n","Ιπποκράτης Κοτσάνης - 131\n","\n","Φιλίτσα-Ιωάννα Κουσκουβέλη - 125\n","\n","\n","\n","----------------------------------------------------------------------------------------------------------------------------------------------------------------\n","PART A"]},{"cell_type":"markdown","metadata":{"id":"C9ZoT716BIHT"},"source":["There are several techniques for handling multi-label classification:\n","\n","1. Binary Relevance: This approach transforms the multi-label problem into multiple binary classification problems. Each label is treated as a separate binary classification task, and a separate classifier is trained for each label independently.\n","\n","2. Classifier Chains: In this approach, a sequence of binary classifiers is created, where each classifier takes into account the predictions of the previous classifiers in the chain as additional features. The order of the classifiers in the chain can be defined randomly or based on the label dependencies.\n","\n","3. Label Powerset: This technique transforms the multi-label problem into a multi-class problem, where each unique combination of labels represents a separate class. This approach requires training a classifier capable of multi-class classification, such as a Decision Tree or Random Forest.\n","\n","4. Adapted Algorithm: Some machine learning algorithms, such as k-Nearest Neighbors (k-NN) or Support Vector Machines (SVM), can be directly extended to handle multi-label classification by modifying their original formulations.\n","-----------------------------------------------------------------------\n","\n","\n","*  The MultiOutputClassifier is used to wrap a base classifier to create a multi-label classifier using the Binary Relevance approach.\n","\n","*  Classifier Chains extend the traditional one-vs-rest approach by considering the order of labels. Each classifier in the chain is trained to predict the presence of a specific label in addition to the labels predicted by the preceding classifiers in the chain. The order of labels is determined by the order in which they appear in the training data.\n","\n","*Note that Classifier Chains can be computationally expensive, especially for a large number of labels, as each label requires training a separate classifier."]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report, zero_one_loss\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.multioutput import MultiOutputClassifier, ClassifierChain\n","from sklearn.pipeline import Pipeline\n","from sklearn.svm import LinearSVC, SVC\n","\n","\n","def txts_to_boW(fileName):\n","    \"\"\"\n","    Takes as input the file's name of our data's file. Each line corresponds to a text.\n","    Each text comprises a different number of sentences. Each sentence comprises\n","    a different number of words.\n","\n","    It returns a list of strings. Each string corresponds to a text's sentences.\n","    Words are separated with simple spaces one from another.\n","    \"\"\"\n","\n","    # Read data from file\n","    with open(fileName, 'r') as f:\n","        lines = f.readlines()\n","\n","    dataset = []\n","    pattern = r'<\\d+>\\s([\\d\\s]+)'\n","    for line in lines:\n","        txt_in_sentences = re.findall(pattern, line)\n","        txt = ''.join(txt_in_sentences)  # join the strings of different sentences to a single one\n","        dataset.append(txt.rstrip())  # strip of any characters in the end\n","    return dataset\n","\n","\n","def get_clfs():\n","    # get a list of models to evaluate\n","    clfs_dict = dict()\n","\n","    # Logistic Regression Classifier with MultiOutputClassifier\n","    lr_params = {'penalty': ['l2'], 'C': [0.1, 1, 10, 100]}\n","    lr_grid = GridSearchCV(LogisticRegression(max_iter=10000), lr_params, cv=5)\n","    lr_clf = MultiOutputClassifier(lr_grid)\n","    clfs_dict['lrMOC'] = lr_clf\n","\n","    # Logistic Regression Classifier with ClassifierChain\n","    lr_params = {'penalty': ['l2'], 'C': [0.1, 1, 10, 100]}\n","    lr_grid = GridSearchCV(LogisticRegression(max_iter=10000), lr_params, cv=5)\n","    lr_clf = ClassifierChain(lr_grid)\n","    clfs_dict['lrCC'] = lr_clf\n","    # --------------------------------------------------------------------------\n","    # Linear SVM Classifier with MultiOutputClassifier\n","    lsvm_params = {'C': [0.1, 1, 10, 100]}\n","    lsvm_grid = GridSearchCV(LinearSVC(max_iter=100000), lsvm_params, cv=5)\n","    lsvm_clf = MultiOutputClassifier(lsvm_grid)\n","    clfs_dict['lsvcMOC'] = lsvm_clf\n","\n","    # Linear SVM Classifier with ClassifierChain\n","    lsvm_params = {'C': [0.1, 1, 10, 100]}\n","    lsvm_grid = GridSearchCV(LinearSVC(max_iter=100000), lsvm_params, cv=5)\n","    lsvm_clf = ClassifierChain(lsvm_grid)\n","    clfs_dict['lsvcCC'] = lsvm_clf\n","\n","    return clfs_dict\n","\n","\n","def apply_clf_models(clfs_dict, X_train, y_train, X_test, y_test):\n","    for clf_name, clf in clfs_dict.items():\n","        if clf_name.startswith('adapted'):\n","            # Adapted algorithm\n","            clf.fit(X_train, y_train)\n","            pred_labels = clf.predict(X_test)\n","        else:\n","            # MultiOutputClassifier\n","            clf.fit(X_train, y_train)\n","            pred_labels = clf.predict(X_test)\n","\n","        print('\\nEvaluating {}'.format(clf_name))\n","        print(classification_report(y_test, pred_labels))\n","        print('Subset accuracy = {}'.format((1 - zero_one_loss(y_test, pred_labels))))\n","\n","\n","def main():\n","    x_train_fileName, x_test_fileName = 'train-data.dat', 'test-data.dat'  # X\n","    y_train_fileName, y_test_fileName = 'train-label.dat', 'test-label.dat'  # y\n","\n","    vectorizer = TfidfVectorizer()\n","\n","    X_train = txts_to_boW(x_train_fileName)\n","    X_test = txts_to_boW(x_test_fileName)\n","\n","    # Create the pipeline\n","    pipeline = Pipeline([\n","        ('vectorizer', vectorizer),\n","        ('clf', None)  # Placeholder for the classifier\n","    ])\n","\n","    # Read y_train data\n","    df_y_train = pd.read_csv(y_train_fileName, sep=\"\\s+\", header=None)\n","    # print(df_y_train)\n","\n","    # Read y_test data\n","    df_y_test = pd.read_csv(y_test_fileName, sep=\"\\s+\", header=None)\n","    # print(df_y_test)\n","\n","    clfs_dict = get_clfs()\n","\n","    for clf_name, clf in clfs_dict.items():\n","        pipeline.set_params(clf=clf)\n","\n","        # Fit the pipeline on training data\n","        pipeline.fit(X_train, df_y_train)\n","\n","        # Predict on the test data\n","        pred_labels = pipeline.predict(X_test)\n","\n","        print('\\nEvaluating {}'.format(clf_name))\n","        print(classification_report(df_y_test, pred_labels))\n","        print('Subset accuracy = {}'.format((1 - zero_one_loss(df_y_test, pred_labels))))\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GL5Wbb052QUQ","outputId":"5ba1d2b4-56d1-4061-88fb-650a63de18a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Evaluating lrMOC\n","              precision    recall  f1-score   support\n","\n","           0       0.79      0.65      0.71       977\n","           1       0.78      0.30      0.44       228\n","           2       0.66      0.40      0.49      1558\n","           3       0.78      0.52      0.62       372\n","           4       0.71      0.37      0.49      1050\n","           5       0.63      0.11      0.19       537\n","           6       0.59      0.12      0.19       702\n","           7       0.78      0.33      0.46      1079\n","           8       0.77      0.24      0.36       803\n","           9       0.70      0.39      0.50       483\n","          10       0.67      0.35      0.46       507\n","          11       0.65      0.28      0.39       478\n","          12       0.71      0.11      0.19       509\n","          13       0.65      0.28      0.40       355\n","          14       0.73      0.38      0.50       392\n","          15       0.55      0.23      0.33       441\n","          16       0.63      0.25      0.36       269\n","          17       0.73      0.29      0.42       501\n","          18       0.82      0.40      0.54       207\n","          19       0.73      0.31      0.43       133\n","\n","   micro avg       0.71      0.33      0.45     11581\n","   macro avg       0.70      0.31      0.42     11581\n","weighted avg       0.70      0.33      0.44     11581\n"," samples avg       0.49      0.33      0.36     11581\n","\n","Subset accuracy = 0.09992467988953047\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Evaluating lrCC\n","              precision    recall  f1-score   support\n","\n","           0       0.79      0.65      0.71       977\n","           1       0.77      0.30      0.43       228\n","           2       0.65      0.40      0.49      1558\n","           3       0.76      0.52      0.62       372\n","           4       0.70      0.39      0.50      1050\n","           5       0.50      0.14      0.22       537\n","           6       0.62      0.12      0.20       702\n","           7       0.75      0.29      0.42      1079\n","           8       0.71      0.30      0.42       803\n","           9       0.70      0.38      0.49       483\n","          10       0.73      0.21      0.33       507\n","          11       0.72      0.25      0.37       478\n","          12       0.61      0.10      0.18       509\n","          13       0.63      0.26      0.37       355\n","          14       0.68      0.40      0.50       392\n","          15       0.59      0.21      0.31       441\n","          16       0.62      0.24      0.34       269\n","          17       0.71      0.30      0.42       501\n","          18       0.76      0.44      0.56       207\n","          19       0.70      0.26      0.38       133\n","\n","   micro avg       0.70      0.33      0.44     11581\n","   macro avg       0.69      0.31      0.41     11581\n","weighted avg       0.69      0.33      0.43     11581\n"," samples avg       0.45      0.32      0.34     11581\n","\n","Subset accuracy = 0.10695455686668343\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Evaluating lsvcMOC\n","              precision    recall  f1-score   support\n","\n","           0       0.85      0.58      0.69       977\n","           1       0.78      0.32      0.45       228\n","           2       0.66      0.38      0.49      1558\n","           3       0.79      0.52      0.63       372\n","           4       0.72      0.36      0.48      1050\n","           5       0.67      0.09      0.15       537\n","           6       0.61      0.09      0.15       702\n","           7       0.80      0.31      0.45      1079\n","           8       0.79      0.23      0.35       803\n","           9       0.70      0.40      0.51       483\n","          10       0.64      0.36      0.46       507\n","          11       0.62      0.28      0.38       478\n","          12       0.77      0.08      0.15       509\n","          13       0.64      0.29      0.40       355\n","          14       0.70      0.38      0.49       392\n","          15       0.54      0.24      0.33       441\n","          16       0.59      0.27      0.37       269\n","          17       0.73      0.31      0.43       501\n","          18       0.85      0.38      0.52       207\n","          19       0.73      0.28      0.40       133\n","\n","   micro avg       0.72      0.32      0.44     11581\n","   macro avg       0.71      0.31      0.41     11581\n","weighted avg       0.71      0.32      0.43     11581\n"," samples avg       0.49      0.31      0.35     11581\n","\n","Subset accuracy = 0.10092894802912378\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Evaluating lsvcCC\n","              precision    recall  f1-score   support\n","\n","           0       0.85      0.58      0.69       977\n","           1       0.77      0.32      0.45       228\n","           2       0.67      0.37      0.48      1558\n","           3       0.76      0.53      0.63       372\n","           4       0.72      0.36      0.48      1050\n","           5       0.55      0.12      0.20       537\n","           6       0.65      0.09      0.15       702\n","           7       0.77      0.27      0.40      1079\n","           8       0.73      0.26      0.38       803\n","           9       0.69      0.39      0.50       483\n","          10       0.79      0.21      0.33       507\n","          11       0.66      0.25      0.37       478\n","          12       0.68      0.08      0.14       509\n","          13       0.70      0.22      0.33       355\n","          14       0.66      0.39      0.49       392\n","          15       0.59      0.21      0.31       441\n","          16       0.63      0.25      0.36       269\n","          17       0.68      0.31      0.43       501\n","          18       0.84      0.43      0.56       207\n","          19       0.89      0.19      0.31       133\n","\n","   micro avg       0.72      0.31      0.43     11581\n","   macro avg       0.71      0.29      0.40     11581\n","weighted avg       0.71      0.31      0.41     11581\n"," samples avg       0.45      0.30      0.33     11581\n","\n","Subset accuracy = 0.10620135576198841\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"markdown","metadata":{"id":"MXZ3Q3CrZojV"},"source":["**General - Comments:**\n","\n","The use of MultiOutputClassifier and ClassifierChain allows us to extend single-label classifiers to handle multi-label classification problems. MultiOutputClassifier treats each label independently and trains separate classifiers for each label. ClassifierChain, on the other hand, takes into account the correlation between labels by incorporating the predictions of previous classifiers as additional features for subsequent classifiers.\n","\n","By using GridSearchCV, we are performing a grid search to find the best combination of hyperparameters for each model. This can help optimize the performance of the models by selecting the most suitable hyperparameters.\n","\n","**Metrics - Comments:**\n","\n","Regarding the four models, we can conclude that:\n","*    MultiOutputClassifier with Logistic Regression:\n","        1. Precision: The average precision across all classes is 0.70. It ranges from 0.59 to 0.82 for individual classes. Precision measures the proportion of true positive predictions among the total predicted positives.\n","        2. Recall: The average recall across all classes is 0.31. It ranges from 0.11 to 0.65 for individual classes. Recall measures the proportion of true positive predictions among the actual positives.\n","        3. F1-score: The average F1-score across all classes is 0.42. It ranges from 0.15 to 0.71 for individual classes. The F1-score combines precision and recall into a single metric, providing a balance between the two.\n","        4. Support: The number of samples in each class.\n","        5. Micro avg: The micro-averaged metrics consider all samples equally and calculate the metrics globally across all classes. The micro-averaged precision, recall, and F1-score are 0.71, 0.33, and 0.45, respectively.\n","        6. Macro avg: The macro-averaged metrics calculate the metrics independently for each class and then take the average. The macro-averaged precision, recall, and F1-score are 0.70, 0.31, and 0.42, respectively.\n","        7. Weighted avg: The weighted-averaged metrics calculate the metrics for each class, weighted by the number of samples in each class, and then take the average. The weighted-averaged precision, recall, and F1-score are 0.70, 0.33, and 0.44, respectively.\n","        8. Samples avg: The sample-based metrics calculate metrics for each instance and then take the average. The sample-based precision, recall, and F1-score are 0.49, 0.33, and 0.36, respectively.\n","        9. Subset accuracy: The subset accuracy is the ratio of samples where all labels are correctly predicted to the total number of samples. It is 0.0999.\n","\n","*    ClassifierChain with Logistic Regression:\n","\n","        The evaluation metrics are similar to the MultiOutputClassifier with Logistic Regression, with slightly lower performance.\n","\n","*    MultiOutputClassifier with LinearSVC:\n","\n","        The precision, recall, and F1-score values are similar to the MultiOutputClassifier with Logistic Regression.\n","\n","*    ClassifierChain with LinearSVC:\n","\n","        The evaluation metrics are also similar to the MultiOutputClassifier with Logistic Regression and ClassifierChain with Logistic Regression.\n","\n","\n","Overall, these results indicate that the classifiers struggle with this multi-label classification task. The precision, recall, and F1-score values are generally low, indicating that the models have difficulty predicting all the correct labels for each sample. The subset accuracy is also low, suggesting that the models struggle to predict all labels accurately for each sample. \n"]},{"cell_type":"markdown","source":["-------------------------------------------------------------------------------\n","In order to be more complete with the assignment, I applied some extra adapted algorithms."],"metadata":{"id":"fvj33zJkumjq"}},{"cell_type":"code","source":["import pandas as pd\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from sklearn.metrics import classification_report, zero_one_loss\n","from sklearn.multioutput import MultiOutputClassifier\n","from sklearn.pipeline import Pipeline\n","\n","\n","def txts_to_boW(fileName):\n","    \"\"\"\n","    Takes as input the file's name of our data's file. Each line corresponds to a text.\n","    Each text comprises a different number of sentences. Each sentence comprises\n","    a different number of words.\n","\n","    It returns a list of strings. Each string corresponds to a text's sentences.\n","    Words are separated with simple spaces one from another.\n","    \"\"\"\n","\n","    # Read data from file\n","    with open(fileName, 'r') as f:\n","        lines = f.readlines()\n","\n","    dataset = []\n","    pattern = r'<\\d+>\\s([\\d\\s]+)'\n","    for line in lines:\n","        txt_in_sentences = re.findall(pattern, line)\n","        txt = ''.join(txt_in_sentences)  # join the strings of different sentences to a single one\n","        dataset.append(txt.rstrip())  # strip of any characters in the end\n","    return dataset\n","\n","\n","def get_clfs():\n","    # get a list of models to evaluate\n","    clfs_dict = dict()\n","\n","    # Gradient Boosting with MultiOutputClassifier\n","    gb = GradientBoostingClassifier()\n","    gb_clf = MultiOutputClassifier(gb)\n","    clfs_dict['gbMOC'] = gb_clf\n","\n","    # Adapted algorithms\n","    adapted_rf = RandomForestClassifier()\n","    clfs_dict['adaptedRF'] = adapted_rf\n","\n","    return clfs_dict\n","\n","\n","def apply_clf_models(clfs_dict, X_train, y_train, X_test, y_test):\n","    for clf_name, clf in clfs_dict.items():\n","        if clf_name.startswith('adapted'):\n","            # Adapted algorithm\n","            clf.fit(X_train, y_train)\n","            pred_labels = clf.predict(X_test)\n","        else:\n","            # MultiOutputClassifier\n","            clf.fit(X_train, y_train)\n","            pred_labels = clf.predict(X_test)\n","\n","        print('\\nEvaluating {}'.format(clf_name))\n","        print(classification_report(y_test, pred_labels))\n","        print('Subset accuracy = {}'.format((1 - zero_one_loss(y_test, pred_labels))))\n","\n","\n","def main():\n","    x_train_fileName, x_test_fileName = 'train-data.dat', 'test-data.dat'  # X\n","    y_train_fileName, y_test_fileName = 'train-label.dat', 'test-label.dat'  # y\n","\n","    vectorizer = TfidfVectorizer()\n","\n","    X_train = txts_to_boW(x_train_fileName)\n","    X_test = txts_to_boW(x_test_fileName)\n","\n","    # Create the pipeline\n","    pipeline = Pipeline([\n","        ('vectorizer', vectorizer),\n","        ('clf', None)  # Placeholder for the classifier\n","    ])\n","\n","    # Read y_train data\n","    df_y_train = pd.read_csv(y_train_fileName, sep=\"\\s+\", header=None)\n","    # print(df_y_train)\n","\n","    # Read y_test data\n","    df_y_test = pd.read_csv(y_test_fileName, sep=\"\\s+\", header=None)\n","    # print(df_y_test)\n","\n","    clfs_dict = get_clfs()\n","\n","    for clf_name, clf in clfs_dict.items():\n","        pipeline.set_params(clf=clf)\n","\n","        # Fit the pipeline on training data\n","        pipeline.fit(X_train, df_y_train)\n","\n","        # Predict on the test data\n","        pred_labels = pipeline.predict(X_test)\n","\n","        print('\\nEvaluating {}'.format(clf_name))\n","        print(classification_report(df_y_test, pred_labels))\n","        print('Subset accuracy = {}'.format((1 - zero_one_loss(df_y_test, pred_labels))))\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"Ht4kTbSUriTC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684261223003,"user_tz":-180,"elapsed":633770,"user":{"displayName":"Ιπποκρατης Κοτσάνης","userId":"08226026588881083056"}},"outputId":"965f0e1b-9863-44fd-ab80-6a0be6b3267e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Evaluating gbMOC\n","              precision    recall  f1-score   support\n","\n","           0       0.80      0.53      0.64       977\n","           1       0.68      0.25      0.37       228\n","           2       0.64      0.25      0.36      1558\n","           3       0.77      0.50      0.61       372\n","           4       0.71      0.33      0.45      1050\n","           5       0.57      0.09      0.16       537\n","           6       0.64      0.13      0.21       702\n","           7       0.74      0.27      0.40      1079\n","           8       0.72      0.24      0.36       803\n","           9       0.75      0.27      0.40       483\n","          10       0.68      0.35      0.47       507\n","          11       0.68      0.15      0.25       478\n","          12       0.64      0.11      0.18       509\n","          13       0.72      0.19      0.31       355\n","          14       0.80      0.28      0.41       392\n","          15       0.66      0.11      0.19       441\n","          16       0.57      0.10      0.17       269\n","          17       0.71      0.21      0.33       501\n","          18       0.65      0.37      0.47       207\n","          19       0.43      0.25      0.32       133\n","\n","   micro avg       0.71      0.26      0.38     11581\n","   macro avg       0.68      0.25      0.35     11581\n","weighted avg       0.69      0.26      0.37     11581\n"," samples avg       0.44      0.26      0.30     11581\n","\n","Subset accuracy = 0.08812452924930958\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Evaluating adaptedRF\n","              precision    recall  f1-score   support\n","\n","           0       0.87      0.41      0.55       977\n","           1       0.94      0.07      0.13       228\n","           2       0.71      0.11      0.20      1558\n","           3       0.96      0.13      0.24       372\n","           4       0.82      0.12      0.20      1050\n","           5       0.70      0.01      0.03       537\n","           6       0.70      0.02      0.04       702\n","           7       0.74      0.07      0.13      1079\n","           8       0.71      0.06      0.11       803\n","           9       0.90      0.06      0.11       483\n","          10       0.84      0.05      0.10       507\n","          11       0.70      0.03      0.06       478\n","          12       0.43      0.01      0.01       509\n","          13       0.91      0.06      0.11       355\n","          14       0.95      0.05      0.09       392\n","          15       0.42      0.01      0.02       441\n","          16       0.67      0.01      0.03       269\n","          17       0.81      0.03      0.05       501\n","          18       0.90      0.04      0.08       207\n","          19       0.60      0.02      0.04       133\n","\n","   micro avg       0.80      0.09      0.16     11581\n","   macro avg       0.76      0.07      0.12     11581\n","weighted avg       0.76      0.09      0.15     11581\n"," samples avg       0.18      0.09      0.11     11581\n","\n","Subset accuracy = 0.064775295003766\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"markdown","source":["**Metrics - Comments:**\n","\n","  * Evaluating gbMOC:\n","\n","      1. Precision, recall, and F1-score: The precision measures the proportion of correctly predicted positive labels, recall measures the proportion of true positive labels correctly predicted, and F1-score is the harmonic mean of precision and recall. Overall, the performance is relatively low for most classes, with varying precision, recall, and F1-score values. Some classes have higher scores (e.g., class 0, 3, 7), indicating better prediction performance, while others have lower scores (e.g., class 5, 12, 15), indicating poorer prediction performance.\n","      2. Support: The number of samples in each class.\n","      3. Micro avg, macro avg, weighted avg, samples avg: These are aggregate metrics calculated across all classes. Micro avg calculates the metrics globally by counting the total true positives, false negatives, and false positives, while macro avg calculates the metrics independently for each class and then takes the average. Weighted avg considers the support (number of samples) for each class in calculating the average. Samples avg calculates metrics for each sample, then averages them.\n","      4. Subset accuracy: Subset accuracy measures the proportion of samples where all the labels are predicted correctly. In this case, the subset accuracy is quite low at 0.088, indicating that the models struggle to accurately predict multiple labels for each sample.\n","\n","*    Evaluating adaptedRF:\n","\n","        1. Precision, recall, and F1-score: Similar to the gbMOC results, the precision, recall, and F1-score values are generally low for most classes. Some classes have relatively higher scores (e.g., class 0, 3, 7), while others have lower scores (e.g., class 5, 12, 15).\n","        2. Support: The number of samples in each class.\n","        3. Micro avg, macro avg, weighted avg, samples avg: These aggregate 3.metrics are calculated similarly to the gbMOC results.\n","        4. Subset accuracy: The subset accuracy is even lower at 0.064, indicating poorer performance in accurately predicting multiple labels for each sample.\n","\n","\n","\n","Overall, both algorithms, gbMOC and adaptedRF, have relatively low performance in predicting multiple labels. \n","\n","**Compared with the above linear models, we can conclude that linear models have better performance and generally metrics.**\n"],"metadata":{"id":"7_vWlCMFrtAe"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}